# ğŸ’³ Credit Card Fraud Detection âœ…

## ğŸ’¼ Business Problem
Banks face significant financial losses due to fraudulent transactions.  
This project aims to build a machine learning model that accurately detects fraud, minimizing false negatives (missed fraud) while keeping false positives (false alarms) low. This reduces manual effort, improves customer trust, and prevents revenue leakage.

## ğŸ¯ Project Aim
- âœ… To develop an efficient, interpretable, and cost-effective fraud detection system.
- ğŸ¯ Focused on achieving high **recall** (to catch more frauds) and high **precision** (to reduce false alarms).
- ğŸ“‰ Reducing financial losses and enhancing operational efficiency for banking systems.

### ğŸ” Evaluation Priorities in Fraud Detection

#### ğŸ¯ Primary Goal: **Minimize False Negatives (FN)**  
**False Negative** = Fraudulent transaction predicted as not fraud.  
âŒ This is very dangerous, because real fraud slips through undetected.  
âœ… **High Recall** helps reduce FNs.  
> ğŸ“Œ **Why it's critical**: A missed fraud costs the bank/customer money.

#### âš–ï¸ Secondary Goal: **Minimize False Positives (FP)**  
**False Positive** = Genuine transaction predicted as fraud.  
â— This causes customer inconvenience (e.g., card gets blocked unnecessarily).  
âœ… **High Precision** helps reduce FPs.  
> ğŸ“Œ **Why it matters**: Too many false alarms hurt customer experience and trust.

---

### ğŸ§  Realistic Objective  
âœ… Prioritize **minimizing false negatives**  
âš–ï¸ While keeping **false positives within acceptable limits**

## âš–ï¸ Class Imbalance Handling  
The dataset was highly imbalanced with **âœ… 99.08% legitimate** and **â—0.16% fraudulent** transactions.  
To address this issue, **ğŸ”„ Random UnderSampling** was applied, resulting in a âœ”ï¸ balanced dataset that enabled fair learning for all models.

## ğŸ“Œ Project Highlights

- **ğŸ” Data Preprocessing**:  
  - Cleaned data and handled missing values  
  - Handled extreme class imbalance via undersampling  
  - Performed feature engineering

- **ğŸ¤– Machine Learning Models Applied**:  
  - âœ… Logistic Regression  
  - ğŸŒ² Decision Tree  
  - ğŸŒ³ Random Forest  
  - âš¡ XGBoost  

- **ğŸ”§ Hyperparameter Tuning**:  
  - Used `GridSearchCV` for each model to optimize performance

- **ğŸ“Š Evaluation Metrics**:  
  - Compared models based on **Precision**, **Recall**, **F1 Score**, and **ROC AUC**

- ğŸ† **Final Model Selected**:  
  - **XGBoost** was chosen due to its **superior performance** across all key metrics, especially ROC AUC (0.97+) and F1 Score (0.92+)

## ğŸ›  Technologies Used

- ğŸ **Python**
- âš™ï¸ **Scikit-learn**
- ğŸ“ˆ **Matplotlib** & **Seaborn** for visualization
- ğŸ§¹ **Data Preprocessing** (cleaning, scaling, encoding)
- âš–ï¸ **Class Balancing** using Random UnderSampling



## ğŸ“¬ If you have any suggestions, feedback, or queries related to this project, feel free to reach out!
ğŸ¤ Contributions, suggestions are welcome!

